{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendaciones proyecto\n",
    "\n",
    "\n",
    "1. **Rendimiento esperado:**  \n",
    "   - No es necesario alcanzar un rendimiento cercano al **100%** para obtener la nota máxima en el proyecto. Se evaluará la calidad del enfoque, la justificación de las decisiones tomadas y la interpretación de los resultados obtenidos.  \n",
    "\n",
    "2. **Uso de muestras en caso de limitaciones:**  \n",
    "   - Si su equipo no puede cargar o procesar todos los datos debido a limitaciones de recursos (como RAM), puede trabajar con **muestras representativas del conjunto de datos**.  \n",
    "   - Es **obligatorio justificar** este enfoque en el informe, explicando claramente las razones detrás de la decisión y cómo se asegura que las muestras son representativas.  \n",
    "\n",
    "3. **Optimización de recursos:**  \n",
    "   - Si enfrenta problemas de memoria al ejecutar tareas, **reduzca la cantidad de procesos paralelos** (**jobs**) a un nivel que su computador o intérprete web pueda manejar. Es preferible priorizar la estabilidad del proceso sobre la velocidad.\n",
    "\n",
    "4. **Paralelización para búsquedas de hiperparámetros:**  \n",
    "   - Aproveche la paralelización para acelerar la búsqueda de hiperparámetros, especialmente si esta es un cuello de botella en su proyecto. Herramientas como `GridSearchCV`, `RandomizedSearchCV` o `Optuna` suelen permitir paralelización configurando el parámetro `n_jobs`.  \n",
    "\n",
    "5. **Grillas de búsqueda razonables:**  \n",
    "   - Al realizar búsquedas de hiperparámetros, **diseñe grillas de búsqueda razonables** que no sean excesivamente grandes.  \n",
    "   - Recuerde que, aunque explorar un mayor espacio de hiperparámetros puede parecer atractivo, también puede hacer que el proceso sea extremadamente lento o inviable. Ajuste el tamaño de las grillas para garantizar que la búsqueda **converja en tiempos razonables** y no tome **\"3.5 eternidades\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A continuación, se detallan las tareas que deben completar para la **Entrega Parcial 1**. Recuerden que el principal entregable es **predecir, con su mejor modelo, sobre los clientes del archivo `X_t1.parquet` y subir las predicciones a la plataforma CodaLab**. **Para esta entrega el desarrollo del informe es sugerido, pero no mandatorio**. Asegúrense de utilizar el archivo correspondiente para subir los resultados a la competencia. El uso de datos incorrectos se reflejará en un bajo desempeño en la tabla de clasificación.\n",
    "\n",
    "### **Tareas a realizar:**\n",
    "1. **Análisis exploratorio de datos:**  \n",
    "   Realicen un análisis detallado para identificar patrones, tendencias y relaciones en los datos. Este paso les permitirá comprender mejor las características del conjunto de datos y guiar las siguientes decisiones en el pipeline de modelamiento.\n",
    "\n",
    "2. **Preprocesamiento de datos:**  \n",
    "   Incluyan técnicas de preprocesamiento que aseguren la calidad y adecuación de los datos para los modelos. Algunas tareas sugeridas son:  \n",
    "   - Estandarización de filas y/o columnas.  \n",
    "   - Reducción de dimensionalidad.  \n",
    "   - Discretización de variables numéricas a categóricas.  \n",
    "   - Manejo de datos nulos.  \n",
    "   - Otras transformaciones relevantes según los datos disponibles.  \n",
    "\n",
    "3. **División del conjunto de datos:**  \n",
    "   Implementen una técnica **Hold-Out**, separando el conjunto de datos en **70% para entrenamiento** y **30% para testeo**.\n",
    "\n",
    "4. **Creación de un modelo baseline:**  \n",
    "   Entrenen un modelo sencillo que sirva como línea base para comparar el rendimiento de los modelos más avanzados.\n",
    "\n",
    "5. **Desarrollo de 3 modelos de Machine Learning diferentes al baseline:**  \n",
    "   - Utilicen exclusivamente pipelines de **Scikit-Learn** para esta iteración del proyecto.  \n",
    "   - El uso de librerías o herramientas externas será penalizado con una calificación de **0** en las secciones implicadas.  \n",
    "\n",
    "6. **Interpretabilidad del modelo con mejores resultados:**  \n",
    "   Apliquen técnicas que permitan interpretar y justificar los resultados obtenidos por el modelo con mejor desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informe Proyecto Lab\n",
    "Integrantes: \n",
    "- Alonso Uribe\n",
    "- Carolina Núñez\n",
    "  \n",
    "Grupo: Adaval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **1. Introducción [0.25 puntos]**  \n",
    "Esta sección debe incluir:  \n",
    "- Una breve descripción del problema planteado: ¿Qué se intenta predecir?  \n",
    "- Un resumen de los datos de entrada proporcionados.  \n",
    "- La métrica seleccionada para evaluar los modelos, justificando su elección. Considerando que los datos están desbalanceados, eviten usar `accuracy` y enfoquen su análisis en métricas como `precision`, `recall` o `f1-score`, indicando en qué clase se centrarán.  \n",
    "- Una mención breve de los modelos utilizados para resolver el problema, incluyendo las transformaciones intermedias aplicadas a los datos.  \n",
    "- Un cierre con un análisis general de los resultados obtenidos, indicando si el modelo final cumplió con los objetivos y cómo se posicionaron respecto a otros equipos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "\n",
    "\n",
    "maxcols = pd.get_option(\"display.max_columns\")\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para entrenar\n",
    "X_t0 = pd.read_parquet('X_t0.parquet')\n",
    "y_t0 = pd.read_parquet('y_t0.parquet')\n",
    "# para predecir(?)\n",
    "X_t1 = pd.read_parquet('X_t1.parquet')\n",
    "X_t0 = X_t0[sorted(X_t0.columns)]\n",
    "X_t1 = X_t1[sorted(X_t1.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", maxcols )\n",
    "X_t0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que no hayan valores nulos\n",
    "X_t0.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var = X_t0.select_dtypes(include=['number']).columns\n",
    "object_var = X_t0.select_dtypes(include=['object']).columns\n",
    "timestamps = pd.Index([col for col in X_t0.columns if \"timestamp\" in col])\n",
    "numeric_var = numeric_var.drop(timestamps)\n",
    "timestamps = timestamps.drop('risky_first_last_tx_timestamp_diff')\n",
    "ts_diff_col = pd.Index(['risky_first_last_tx_timestamp_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t0[timestamps].map(lambda ms: datetime.datetime.fromtimestamp(ms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que las columnas timestamp hablan de fechas de transacción. Sin conocimiento del negoción, hablan de una fecha inicio y termino del prestamo(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_t0[ts_diff_col[0]] == (X_t0['risky_last_tx_timestamp']) - X_t0['risky_first_tx_timestamp']).sum()/ len(X_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí podemos ver que la columna `risky_first_last_tx_timestamp_diff` o corresponde a la diferencia entre las columnas `risky_last_tx_timestamp` y `risky_first_tx_timestamp`.  \n",
    "Por lo tanto debemos decidir que necesitamos del fenómeno, los valores absolutos en el tiempo, o solo la diferencia de estos valores. Concluimos que estos tres valores junto no entraran al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca las variables binarias a partir del número de valores distintos que tenga\n",
    "weird = defaultdict(list)\n",
    "threshold = 0.5\n",
    "for col in X_t0[numeric_var]:\n",
    "    diff_values = len(X_t0[col].value_counts())\n",
    "    min_prop = (X_t0[col] == X_t0[col].min()).sum() / len(X_t0[col]) \n",
    "    max_prop = (X_t0[col] == X_t0[col].max()).sum() / len(X_t0[col])\n",
    "    is_binary = diff_values == 2\n",
    "    is_weird = False\n",
    "    if min_prop > threshold and not is_binary :\n",
    "        weird['min_prop'].append(col)\n",
    "        is_weird = True\n",
    "    if max_prop > threshold and not is_binary :\n",
    "        weird['max_prop'].append(col)\n",
    "        is_weird = True\n",
    "    if is_binary:\n",
    "        weird['binary'].append(col)\n",
    "        is_weird = True\n",
    "    if not is_weird:\n",
    "        weird['not'].append(col)\n",
    "    print(f\"{col}:\\n # valores distintos: {diff_values}\\n Proporción de valor mínimo: {min_prop}\\n Proporción de valor máximo: {max_prop};\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, keys in weird.items():\n",
    "    print(f\"{name.upper()}:\\n {', '.join(sorted(keys))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "X_t0[weird['max_prop']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t0[weird['min_prop']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de Anderson\n",
    "\n",
    "Para determinar que variables se comportan de manera gaussiana, se implementará el test de D'Agostino and Pearson con nivel de significancia del 95%, cuya hipótesis nula es que los datos provienen de una distribución dada. En este caso, de una distribución normal. Entonces, si el p-value es menor al nivel de significancia se rechaza la nula, es decir, los datos no provienen de una distribución normal.\n",
    "\n",
    "(Este test es similar al Test de Kolmogorov-Smirnov, sin embargo funciona bien para muestras grandes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson(np.random.normal(0, 1, len(X_t0['market_cmo'])), dist='norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstest(data[numeric_var[1]], 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D'Agostino and Pearson test\n",
    "from scipy.stats import kstest, norm, anderson, normaltest\n",
    "\n",
    "# Test de normalidad\n",
    "data = X_t0[numeric_var].copy()\n",
    "\n",
    "normal_var = []\n",
    "for var in numeric_var:\n",
    "    statistic, p_value = normaltest(data[var])\n",
    "    # print(f'Variable: {var}\\n statistic: {statistic}    p_value: {p_value}\\n')\n",
    "    if p_value >= 0.05 : # No se rechaza la nula -> Los datos distribuyen normal\n",
    "        normal_var.append(var)\n",
    "\n",
    "normal_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sorpresa nuestra, según los test realizados, ninguna de las variables numéricas distribuye normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "# plt.hist(X_t0['market_apo'], color='blue', label='market_apo');\n",
    "plt.hist((X_t0['market_apo']-X_t0['market_apo'].mean())/X_t0['market_apo'].std(), color='blue', label='market_apo');\n",
    "plt.hist(np.random.normal(0, 1, len(X_t0['market_cmo'])), alpha=0.5, color='red', label='normal');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que estas variables tienen datos atípicos para una variable numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var = numeric_var.drop(weird['binary'])\n",
    "to_datetime_func = FunctionTransformer(lambda df: df.map(lambda ms: datetime.datetime.fromtimestamp(ms)))\n",
    "\n",
    "first_transformer = ColumnTransformer([\n",
    "    ('numeric', StandardScaler(), numeric_var),\n",
    "    ('binary', MinMaxScaler(), weird['binary']),\n",
    "    ('ts', to_datetime_func, timestamps), # ms a Timestamp según fecha POSIX\n",
    "    ('tsdiff', FunctionTransformer(lambda ms: ms/1000), ts_diff_col) # ms a segundos\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "first_transformer.set_output(transform='pandas')\n",
    "optimus = first_transformer.fit_transform(X_t0)\n",
    "optimus.columns = [col.split('_', 1)[-1].lstrip('_') for col in optimus.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = object_var.insert(len(object_var), 'risky_first_tx_timestamp')\n",
    "for _ ,df in optimus[vars].groupby(*object_var):\n",
    "    print(df['risky_first_tx_timestamp'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_qqplot(df, columns, num_cols=7, dist='norm'):\n",
    "    columns = columns if not isinstance(columns, str) else [columns]\n",
    "    num_rows = int(len(columns)/num_cols)+min(1, len(columns)%num_cols)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols if num_rows != 1 else len(columns), sharex=True)\n",
    "    axes = axes.reshape(-1) if isinstance(axes, type(np.array([]))) else [axes]\n",
    "    fig.set_figwidth(26 if num_rows > 1 else 3*len(columns))\n",
    "    fig.set_figheight(num_rows*3)\n",
    "    for i, col in enumerate(columns):\n",
    "        data, _ = stats.probplot(df[col], dist=dist, plot=axes[i])\n",
    "        axes[i].set_title(col)\n",
    "        if i!=0:\n",
    "            axes[i].set_ylabel('')\n",
    "        if isinstance(axes, type(np.array([]))) and i!=(len(axes)-1):\n",
    "            axes[i].set_xlabel('')\n",
    "    fig.set\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qqplot(optimus, numeric_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qqplot(optimus, ts_diff_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qqplot(optimus, weird['binary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qqplot esperado para varible binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qqplot(optimus, weird['max_prop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este campo se asumirá como variable binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(weird['min_prop']))\n",
    "plot_qqplot(optimus, weird['min_prop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar en las primeras 5 variables un comportamiento parecido a una variable binaria. Para facilitar la predicción al modelo, podemos definir para ellas 3 o 4 bins, donde los bins 2 y 3 serían valores de transición desde los menores a los mayores valores encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las demás variables, vemos comportamientos interesantes. Por ejemplo para `max_market_drawdown_365d` se observan valores escalonados, lo que también podría permitirnos simplificar estos valores en bins. A su vez, vemos que varias se comportan de manera normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", maxcols)\n",
    "# Histogramas para revisar el comportamiento de las variable\n",
    "axes = X_t0[X_t0.drop(columns=weird['not']).columns].hist(figsize=(16,12), bins=15)\n",
    "for ax in axes.flatten():\n",
    "    ax.set_title(ax.get_title(), fontsize=10)\n",
    "    ax.tick_params(axis='x', labelsize=6)\n",
    "    ax.tick_params(axis='y', labelsize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = X_t0[weird['not']].hist(figsize=(20,15), bins=15)\n",
    "for ax in axes.flatten():\n",
    "    ax.set_title(ax.get_title(), fontsize=10)\n",
    "    ax.tick_params(axis='x', labelsize=6)\n",
    "    ax.tick_params(axis='y', labelsize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr = X_t0[weird['not']].corr().stack()\n",
    "X_corr = X_corr[X_corr.index.get_level_values(0) != X_corr.index.get_level_values(1)]\n",
    "X_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr = X_t0[weird['not']].corr()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x = X_corr.columns,\n",
    "        y = X_corr.index,\n",
    "        z = np.array(X_corr),\n",
    "        text=X_corr.values,\n",
    "        texttemplate='%{text:.2f}'\n",
    "    )\n",
    ")\n",
    "fig.update_layout(title_text='Correlación entre las variables numéricas',\n",
    "                    height=800, width=1400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Modelos con Scikit-Learn (Entrega Parcial 1)**\n",
    "\n",
    "##### **2.1. Análisis Exploratorio de Datos [0.5 puntos]**  \n",
    "Realicen un análisis que explore patrones, tendencias y relaciones clave en los datos. Incluyan:  \n",
    "- Estadísticas descriptivas generales.  \n",
    "- Visualizaciones para identificar distribuciones, valores atípicos y posibles relaciones entre variables.  \n",
    "- Cualquier observación relevante que pueda influir en las etapas posteriores del proyecto.  \n",
    "\n",
    "El informe debe ser detallado y profesional, demostrando no solo la implementación técnica, sino también una comprensión profunda de los datos y la problemática planteada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.2 Preprocesamiento de Datos [0.25 puntos]**\n",
    "\n",
    "Esta sección se centra en la limpieza y preparación de los datos para garantizar que sean adecuados para el entrenamiento y evaluación de los modelos. Es fundamental ejecutar un **`train_test_split`** para dividir los datos en conjuntos de entrenamiento y validación, siguiendo la proporción establecida (por ejemplo, 70/30). \n",
    "\n",
    "Se espera la implementación de diversas técnicas de preprocesamiento, tales como:  \n",
    "- **Uso de `ColumnTransformer`:** Permite aplicar transformaciones específicas a diferentes columnas de manera eficiente.  \n",
    "- **Imputación de valores nulos:** Elija una estrategia adecuada (media, mediana, moda, etc.) para completar los datos faltantes.  \n",
    "- **Discretización de variables:** Convierte variables numéricas continuas en categóricas, si resulta útil para el modelo.  \n",
    "- **Estandarización o normalización:** Mejora el rendimiento de algunos algoritmos que son sensibles a la escala de los datos.  \n",
    "- Otras transformaciones necesarias dependiendo de las características específicas del conjunto de datos.\n",
    "\n",
    "El proceso debe estar bien documentado y justificado en el informe, explicando las decisiones tomadas en función de los datos y los objetivos del proyecto.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t0.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que las clases están en proporciones relativamente similares, no consideramos necesario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(optimus, y_t0.values, \n",
    "                                                    test_size=0.3, random_state=29, shuffle=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.3 Baseline [0.25 puntos]**\n",
    "\n",
    "En esta sección se debe construir el modelo más sencillo posible que pueda resolver el problema planteado, conocido como **modelo baseline**. Su propósito es servir como referencia para comparar el rendimiento de los modelos más avanzados desarrollados en etapas posteriores.  \n",
    "\n",
    "Pasos requeridos:  \n",
    "- Implemente, entrene y evalúe un modelo básico utilizando un pipeline.  \n",
    "- Asegúrese de incluir en el pipeline las transformaciones del preprocesamiento realizadas previamente junto con un clasificador básico.  \n",
    "- Evalúe el modelo y presente el informe de métricas utilizando **`classification_report`**.  \n",
    "\n",
    "Documente claramente cómo se creó el modelo, las decisiones tomadas y los resultados obtenidos. Este modelo será la base comparativa en las secciones posteriores.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplico labelEncoder a la col 'wallet_address'\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(X_t0.wallet_address)\n",
    "\n",
    "X_train.wallet_address  = encoder.transform(X_train.wallet_address)\n",
    "X_test.wallet_address = encoder.transform(X_test.wallet_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_dummy = dummy_clf.predict(X_test)\n",
    "\n",
    "# dummy.score:Return the mean accuracy on the given test data and labels.\n",
    "dummy_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(random_state=29)\n",
    "lsvc.fit(X_train.drop(columns=timestamps), y_train)\n",
    "y_lsvc = lsvc.predict(X_test.drop(columns=timestamps))\n",
    "lsvc.score(X_test.drop(columns=timestamps), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC curve\n",
    "models = ['Dummy clf', 'LinearSVC']\n",
    "\n",
    "for i, y in enumerate([y_dummy, y_lsvc]):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y, pos_label=2)\n",
    "    auc(fpr, tpr)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y,)\n",
    "    roc_auc = roc_auc_score(y_test, y)\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'Curva ROC {models[i]}(area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_dummy))\n",
    "print(classification_report(y_test, y_lsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc.score(X_test.drop(columns=timestamps), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision-recall curve shows the tradeoff between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision. High precision is achieved by having few false positives in the returned results, and high recall is achieved by having few false negatives in the relevant results. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all relevant results (high recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = lsvc.decision_function(X_test.drop(columns=timestamps))\n",
    "\n",
    "display = PrecisionRecallDisplay.from_predictions(\n",
    "    y_lsvc, y_score, name=\"LinearSVC\", plot_chance_level=True\n",
    ")\n",
    "_ = display.ax_.set_title(\"2-class Precision-Recall curve\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
