{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyPTffTLug7i"
      },
      "source": [
        "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
        "\n",
        "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pbWVyntzbvL"
      },
      "source": [
        "### **Cuerpo Docente:**\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
        "- Auxiliar: Eduardo Moya\n",
        "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6ikgVYzghB"
      },
      "source": [
        "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados**\n",
        "\n",
        "- Nombre de alumno 1: Carolina Nuñez\n",
        "- Nombre de alumno 2: Alonso Uribe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMJ-owchzjFf"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Repositorio💻](https://github.com/carinunez/Labs_MDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUuwsXrKzmkK"
      },
      "source": [
        "## **Temas a tratar**\n",
        "\n",
        "- Reinforcement Learning\n",
        "- Large Language Models\n",
        "\n",
        "## **Reglas:**\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Prohibidas las copias.\n",
        "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
        "\n",
        "### **Objetivos principales del laboratorio**\n",
        "\n",
        "- Resolución de problemas secuenciales usando Reinforcement Learning\n",
        "- Habilitar un Chatbot para entregar respuestas útiles usando Large Language Models.\n",
        "\n",
        "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmHHQ9BuyAG"
      },
      "source": [
        "## **1. Reinforcement Learning (2.0 puntos)**\n",
        "\n",
        "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOcejYb6uzOO"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq gymnasium stable_baselines3\n",
        "!pip install -qqq swig\n",
        "!pip install -qqq gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBPet_Mq8dX9"
      },
      "source": [
        "### **1.1 Blackjack (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
        "\n",
        "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpZ8bBKk9ZlU"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Action Space:\", env.action_space)\n",
        "print(\"Obs Space:\", env.observation_space)"
      ],
      "metadata": {
        "id": "s8m-26MyuyM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.reset())\n"
      ],
      "metadata": {
        "id": "iY-3ieo7uz5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action = env.action_space.sample()\n",
        "print(\"Sample Action:\", action)\n",
        "print(\"Sample Observation:\", env.step(1))"
      ],
      "metadata": {
        "id": "pSoaYYj3u1at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ6J1_-Y9nHO"
      },
      "source": [
        "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5i1Wt1p770x"
      },
      "source": [
        "Las observaciones entregadas se componen de una tupla, en seguida se describen por posición.  \n",
        "Los estados son discretos y se tienen $32 \\cdot 11 \\cdot 2$ en total correspondiente a todas las combinaciones de estas tres variables.\n",
        "* 0: suma de las dos cartas iniciales del jugador\n",
        "* 1: valor de carta boca arriba del repartidor (1-10 con un Ace)\n",
        "* 2: Si el jugador posee un Ace\n",
        "\n",
        "La acción es un valor 0 o 1 (int)\n",
        "* 0: Seguir sacando cartas para acercarse a la suma 21\n",
        "* 1: Retirarse\n",
        "\n",
        "Las recompensas (int)\n",
        "* 1: gana\n",
        "* 0: empata\n",
        "* -1: pierde\n",
        "* 1.5: ganando con Blackjack natural (Ace + J, Q o K). Opcional.\n",
        "\n",
        "Si el juego ha terminado (bool); ganando, perdiendo o empatando"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmcX6bRC9agQ"
      },
      "source": [
        "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p2PrLLR9yju"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "epochs = 5000\n",
        "rewards = []\n",
        "for epoch in range(epochs):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.random.randint(0,2) # acción aleatoria\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "    # Solo queremos la recompensa\n",
        "    rewards.append(reward)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = pd.Series(rewards)\n",
        "print(\"Media:\", rewards.mean())\n",
        "print(\"Desviación:\", rewards.std())"
      ],
      "metadata": {
        "id": "shFpXoMHvAWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es pésima la recompensa. Hay mayor cantidad de juegos perdidos a ganados, y la desviación es casi tan grande como diferencia entre empatar y ganar, es decir, el modelo no pierde y empata consistentemente (también tendríamos una media de [-0.4,-0.5]), es más aleatorio que eso."
      ],
      "metadata": {
        "id": "AClvYlpmvDFC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEO_dY4x_SJu"
      },
      "source": [
        "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9JsFA1wGmnH"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3.common.utils import random\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "random.seed(29)\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=32000)\n",
        "model.save(\"ppo_blackjack\")\n",
        "\n",
        "del model # remove to demonstrate saving and loading\n",
        "\n",
        "model = PPO.load(\"ppo_blackjack\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-bpdb8wZID1"
      },
      "source": [
        "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-d7d8GFf7F6"
      },
      "outputs": [],
      "source": [
        "epochs = 10000\n",
        "rewards = []\n",
        "for epoch in range(epochs):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "    # Solo queremos la recompensa\n",
        "    rewards.append(reward)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = pd.Series(rewards)\n",
        "print(\"Media:\", rewards.mean())\n",
        "print(\"Desviación:\", rewards.std())"
      ],
      "metadata": {
        "id": "iDEmAQpWvXyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es bastante mejor, ahora se pierde y se gana casi con la misma probabilidad, difícilmente se empatada dada la varianza tan alta. Aún así no sirve para hacerse millonario. Puede ser porque este juego esta sujeto en gran medida por el azar, aún así, el agente es capaz de equiparar las probabilidades de ganar y perder."
      ],
      "metadata": {
        "id": "mVx9PCorvb2P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO-EsAaPAYEm"
      },
      "source": [
        "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
        "\n",
        "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
        "\n",
        "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
        "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
        "\n",
        "¿Son coherentes sus acciones con las reglas del juego?\n",
        "\n",
        "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh8XlGyzwtRp"
      },
      "outputs": [],
      "source": [
        "def explicit_player(suma:int, dealer:int, ace:bool):\n",
        "    obs = np.array([suma, dealer, int(ace)])\n",
        "    return \"Hit\" if model.predict(obs)[0] else \"Stick\"\n",
        "\n",
        "tries = 10\n",
        "uno = \"Si la suma de cartas del agente es 6, el dealer muestra un 7, agente no tiene un As:\\n\"\n",
        "dos = \"Si la suma de cartas del agente es 19, el dealer muestra un 3, agente tiene un As:\\n\"\n",
        "print(uno)\n",
        "for _ in range(tries):\n",
        "    print(explicit_player(suma=6, dealer=7, ace=False), end=' ')\n",
        "print('\\n\\n')\n",
        "print(dos)\n",
        "for _ in range(tries):\n",
        "    print(explicit_player(suma=19, dealer=3, ace=True), end=' ')\n",
        "\n",
        "tries = 10000\n",
        "one = []\n",
        "two = []\n",
        "for _ in range(tries):\n",
        "    one.append(model.predict(np.array([6, 7, 0]))[0])\n",
        "    two.append(model.predict(np.array([19, 3, 1]))[0])\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "one = pd.Series(one)\n",
        "two = pd.Series(two)\n",
        "print(uno)\n",
        "print(\"Media:\", one.mean())\n",
        "print(\"Desviación:\", one.std())\n",
        "print('\\n')\n",
        "print(dos)\n",
        "print(\"Media:\", two.mean())\n",
        "print(\"Desviación:\", two.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se concluye que las acciones son coherentes con las reglas del juego. Estado muy cerca del 21 resulta más obvio querer quedarse (Stick) en el número. Lo que se muestra en la media y la desviación del agente para este escenario, rondando siempre esta acción.\n",
        "\n",
        "Para el primer caso resulta más extraña la jugada y esta mucho más sujeta a la suerte que tenga el dealer, es decir, dada la mala mano del agente, este decide apostar por que el repartidor supere el 21, dandole la victoria. Esto se refleja en la media encontrada y su desviación. A pesar de esto existe una desviación estandar superior al otro escenario, esto porque se entiende que el numero 6 sigue siendo bajo y las probabilidades de superar el 21 con la siguiente carta son nulas.\n",
        "\n",
        "Los estados son clase numpy.array"
      ],
      "metadata": {
        "id": "igGfkgPGviiX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEqCTqqroh03"
      },
      "source": [
        "### **1.2 LunarLander**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
        "\n",
        "Comencemos preparando el ambiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\", continuous=True, max_episode_steps=1000) # notar el parámetro continuous = True}\n",
        "# Tuve un error con continuous=True. El método de step() no funcionaba bien. Cambiando a discreto lo arreglo."
      ],
      "metadata": {
        "id": "zF7Q2hxgvteS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBU4lGX3wpN6"
      },
      "source": [
        "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
        "\n",
        "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRiWpSo9yfr9"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def export_gif(model, n = 5):\n",
        "  '''\n",
        "  función que exporta a gif el comportamiento del agente en n episodios\n",
        "  '''\n",
        "  images = []\n",
        "  for episode in range(n):\n",
        "    obs = model.env.reset()\n",
        "    img = model.env.render()\n",
        "    done = False\n",
        "    while not done:\n",
        "      images.append(img)\n",
        "      action, _ = model.predict(obs)\n",
        "      obs, reward, done, info = model.env.step(action)\n",
        "      img = model.env.render(mode=\"rgb_array\")\n",
        "\n",
        "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk5VJVppXh3N"
      },
      "source": [
        "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
        "\n",
        "Nota: recuerde que se especificó el parámetro `continuous = True`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-u9LUE8O9a"
      },
      "source": [
        "Los estados son continuos y por lo tanto son teoricamente infinitos.\n",
        "\n",
        "Las observaciones entregadas se componen de una tupla de ocho valores, 6 continuos para nuestro experimento, y dos boolean\n",
        "\n",
        "* 0: Posición eje X (float)\n",
        "* 1: Posición eje Y (float)\n",
        "* 2: Velocidad eje X (float)\n",
        "* 3: Velocidad eje Y (float)\n",
        "* 4: Angulo de la nave (float)\n",
        "* 5: Velocidad Angular (float)\n",
        "* 6: Pata 1 en contacto con las plataforma de aterrizaje (int)\n",
        "* 7: Pata 2 en contacto con las plataforma de aterrizaje (int)\n",
        "\n",
        "La acción es un valor 0 al 3 (int)\n",
        "* 0: No acción\n",
        "* 1: Prender motor izquierdo\n",
        "* 2: Prender motor central\n",
        "* 3: Prender motor derecho\n",
        "\n",
        "Las recompensas (int)\n",
        "\n",
        "* Aumenta/decrece de acuerdo a la cercania/lejania de la nave a la plataforma de aterrizaje.\n",
        "* Aumenta/decrece según mayor/menor sea su velocidad.\n",
        "* Decrece si la nave se ladea (a mayor angulo).\n",
        "* 10: por cada pata tocando la plataforma.\n",
        "* 0.03: por cada cuadro donde alguno de los motores laterales estan encendidos.\n",
        "* 0.3: por cada cuadro donde el motor central esta encendido.\n",
        "* +100/-100 por aterrizar exitosamente/estrellarse.\n",
        "\n",
        "Juego terminado\n",
        "\n",
        "* La nave se estrella.\n",
        "* La plataforma de aterrizaje sale del cuadro (X>1).\n",
        "* La nave no está \"despierta\". Según los documentos de Box2D, un cuerpo que no está despierto es un cuerpo que no se mueve y no choca con ningún otro cuerpo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YChodtNQwzG2"
      },
      "source": [
        "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bwc3A0GX7a8"
      },
      "outputs": [],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_6Ia9uoF7Hs"
      },
      "outputs": [],
      "source": [
        "env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "rewards = []\n",
        "for epoch in range(epochs):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        # action, _ = model.predict(obs)\n",
        "        action = np.random.randint(0,4)\n",
        "        obs, r, done, _, _ = env.step(env.action_space.sample())\n",
        "        reward += r\n",
        "    # Solo queremos la recompensa\n",
        "    rewards.append(reward)"
      ],
      "metadata": {
        "id": "r92YrDs5wcfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = pd.Series(rewards)\n",
        "print(\"Media:\", rewards.mean())\n",
        "print(\"Desviación:\", rewards.std())"
      ],
      "metadata": {
        "id": "QogD1msoweRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mala. Que la media sea casi el doble que el puntaje obtenido al estrellarse, es de suponer que la gran mayoría de los episodios la nave se estrello"
      ],
      "metadata": {
        "id": "2mEPV971whfx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQrZVQflX_5f"
      },
      "source": [
        "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ophyU3KrWrwl"
      },
      "outputs": [],
      "source": [
        "def train_ppo(timestepts=10000, lr=0.0003, batch_size=64, save=True):\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=lr, batch_size=batch_size)\n",
        "    model.learn(total_timesteps=timestepts, )\n",
        "    if save:\n",
        "        model.save(\"ppo_lunarlander\")\n",
        "    return model\n",
        "train_ppo()\n",
        "model = PPO.load(\"ppo_lunarlander\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z-oIUSrlAsY"
      },
      "source": [
        "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "rewards = []\n",
        "for epoch in range(epochs):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        # action, _ = model.predict(obs)\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, r, done, _, _ = env.step(action)\n",
        "        reward += r\n",
        "    # Solo queremos la recompensa\n",
        "    rewards.append(reward)"
      ],
      "metadata": {
        "id": "mJPGs60gxPOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = pd.Series(rewards)\n",
        "print(\"Media:\", rewards.mean())\n",
        "print(\"Desviación:\", rewards.std())"
      ],
      "metadata": {
        "id": "WGsnEE0VxSsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es mejor, pero no mucho mejor. La media es mayor pero igual cercana a la recompensa por estrellar (-100). La desviación es menor, se puede asumir que hubieron algunos episodios donde la nave pudo mantenerse y/o acercarse a la plataforma de aterrizaje antes de estrellar."
      ],
      "metadata": {
        "id": "OhLRNwtZxXQ0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Xw4YHT3P5d"
      },
      "source": [
        "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
        "\n",
        "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
        "- `total_timesteps`\n",
        "- `learning_rate`\n",
        "- `batch_size`\n",
        "\n",
        "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
        "\n",
        "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aItYF6sr6F_6"
      },
      "outputs": [],
      "source": [
        "model = train_ppo(timestepts=100000, lr=0.0001, batch_size=128, save=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "rewards = []\n",
        "for epoch in range(epochs):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        # action, _ = model.predict(obs)\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, r, done, _, _ = env.step(action)\n",
        "        reward += r\n",
        "    # Solo queremos la recompensa\n",
        "    rewards.append(reward)"
      ],
      "metadata": {
        "id": "vISMn7Wmxe1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = pd.Series(rewards)\n",
        "print(\"Media:\", rewards.mean())\n",
        "print(\"Desviación:\", rewards.std())"
      ],
      "metadata": {
        "id": "afUjlxu9xg0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_gif(model)"
      ],
      "metadata": {
        "id": "TJYhfFgZxiug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPUY-Ktgf2BO"
      },
      "source": [
        "## **2. Large Language Models (4.0 puntos)**\n",
        "\n",
        "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ4fPRRihGLe"
      },
      "source": [
        "### **2.0 Configuración Inicial**\n",
        "<!--\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
        "\" width=\"400\">\n",
        "</p> -->\n",
        "\n",
        "Como siempre, cargamos todas nuestras API KEY al entorno:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud2Xm_k-hFJn"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "\n",
        "if \"TAVILY_API_KEY\" not in os.environ:\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj9JvQUsgZZJ"
      },
      "source": [
        "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
        "\n",
        "<!-- <p align=\"center\">\n",
        "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
        "\" width=\"400\">\n",
        "</p> -->\n",
        "\n",
        "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxOQroVnaZ5"
      },
      "source": [
        "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
        "\n",
        "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
        "  - 2 documentos .pdf como mínimo.\n",
        "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
        "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
        "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
        "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
        "  - **Recuerden adjuntar los documentos en su entrega**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "pH-I97c5wPUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D1tIRCi4oJJ"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-google-genai"
      ],
      "metadata": {
        "id": "caUD7ufNwfhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet faiss-cpu langchain_community pypdf"
      ],
      "metadata": {
        "id": "Io4tl9IgyVAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzq2TjWCnu15"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "doc_paths = ['/content/gdrive/MyDrive/pdf_LLM/506.pdf',\n",
        "             '/content/gdrive/MyDrive/pdf_LLM/La-guia-peq-cambios-castella.pdf',\n",
        "             '/content/gdrive/MyDrive/pdf_LLM/Manual_basico_N_clinica_y_Dietetica_Valencia_2012.pdf',\n",
        "             '/content/gdrive/MyDrive/pdf_LLM/food-for-thought-mental-health-nutrition-briefing-march-2017.pdf',\n",
        "             '/content/gdrive/MyDrive/pdf_LLM/nutrients-14-00750.pdf'] # rellenar con los path a sus documentos\n",
        "\n",
        "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
        "\n",
        "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
        "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r811-P71nizA"
      },
      "source": [
        "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
        "\n",
        "Vectorice los documentos y almacene sus representaciones de manera acorde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-yXAdCSn4JM"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "docs = PyPDFLoader(doc_paths[-1]).load()\n",
        "\n",
        "for doc in doc_paths[:-1]:\n",
        "  new_doc = PyPDFLoader(doc).load()\n",
        "  docs.extend(new_doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits docs in chunks\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) # inicializamos splitter\n",
        "splits = text_splitter.split_documents(docs) # dividir documentos en chunks"
      ],
      "metadata": {
        "id": "XV1spUnLzL8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding) # vectorizacion y almacenamiento\n",
        "vectorstore"
      ],
      "metadata": {
        "id": "kE8h7lFIzGmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAUkP5zrnyBK"
      },
      "source": [
        "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
        "\n",
        "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPIySdDFn99l",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", # método de búsqueda\n",
        "                                     search_kwargs={\"k\": 3}, # n° documentos a recuperar\n",
        "                                     )\n",
        "\n",
        "# Funcion para corregir formato\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "retriever_chain = retriever | format_docs # chain\n",
        "print(retriever_chain.invoke(\"lista de verduras con mayor contenido en fibra\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\", # modelo de lenguaje\n",
        "    temperature=0.2, # probabilidad de \"respuestas creativas\"\n",
        "    max_tokens=None, # sin tope de tokens\n",
        "    timeout=None, # sin timeout\n",
        "    max_retries=2, # número máximo de intentos\n",
        ")\n",
        "\n",
        "# Entrego contexto para las respuestas\n",
        "rag_template = '''\n",
        "Eres un asistente experto en nutrición y salud. Haces recomendaciones sobre alimentos\n",
        "a consumir según el contexto que se te entrega.\n",
        "Tu único rol es contestar preguntas del usuario a partir de información relevante que te sea proporcionada.\n",
        "Responde siempre de la forma más completa posible y usando toda la información entregada.\n",
        "Responde sólo lo que te pregunten a partir de la información relevante, NUNCA inventes una respuesta.\n",
        "Responde en español, a menos que se te indique lo contrario.\n",
        "\n",
        "Información relevante: {context}\n",
        "Pregunta: {question}\n",
        "Respuesta útil:\n",
        "'''\n",
        "rag_prompt = PromptTemplate.from_template(rag_template)\n",
        "\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
        "        \"question\": RunnablePassthrough(), # question pasará directo hacia el prompt\n",
        "    }\n",
        "    | rag_prompt # prompt con las variables question y context\n",
        "    | llm # llm recibe el prompt y responde\n",
        "    | StrOutputParser() # recuperamos sólo la respuesta\n",
        ")\n",
        "\n",
        "question = \"que frutas recomiendas para la deshidratación?\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "W7Ar_lIFWuM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"entregame 5 verduras con alto contenido en fibra y su cantidad, si dispones de ella'\"\n",
        "# response = rag_chain.invoke(question)\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "Yhcm2MgSeABQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"que alimentos recomeindas para reducir la obesidad\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "TRtc383vcfAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke('recomendaciones de meriendas dulces'))"
      ],
      "metadata": {
        "id": "aErx77-GMaBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycg5S5i_n-kL"
      },
      "source": [
        "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
        "\n",
        "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
        "\n",
        "Ejemplo de tupla:\n",
        "- Pregunta: ¿Quién es el presidente de Chile?\n",
        "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_UiEn1hoZYR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# tomé como referencia, respuestas encontradas en internet\n",
        "referencia = [\n",
        "    ({'pregunta':'que alimentos debe tener la dieta de una persona con diabetes?'},\n",
        "      {'respuesta':'Frutas.Verduras.Granos integrales.Legumbres, como frijoles y \\\n",
        "      guisantes. Productos lácteos bajos en grasa, como leche y queso.'}),\n",
        "\n",
        "    ({'pregunta':'alimentos recomendados para la deshidratación'},\n",
        "     {'respuesta': \"Beba líquidos. A veces, los líquidos muy fríos resultan más fáciles de beber.\\\n",
        "                    Recuerde que los alimentos contienen líquidos. Procure comer frutas, verduras,\\\n",
        "                    sopas, gelatinas, paletas de helado y otros alimentos hidratados\"}),\n",
        "    ({'pregunta':'que alimentos recomindas para reducir la obesidad'},\n",
        "     {'respuesta':'Incluir en la dieta 2-3 raciones de fruta al día, repartidas durante la jornada tanto como postres, \\\n",
        "     tentempiés o completando un buen desayuno. Incluir 2 raciones de verduras crudas o cocidas al día. \\\n",
        "     Seleccionar proteínas magras (pescados blancos, carnes blancas) en cocciones sencillas, como a la \\\n",
        "     plancha, brasa, horno, papillote, hervidos, vapor o airfryer. Disminuir las cantidades de sal y \\\n",
        "      azúcar añadido a los alimentos. Beber por lo menos entre 1,5 y 2 litros de agua al día. \\\n",
        "      Planificar como mínimo 5 menús a la semana para evitar picoteos u otras opciones menos saludables.'})]\n",
        "\n",
        "for i, ref in enumerate(referencia):\n",
        "  print('Pregunta: ', ref[0]['pregunta'])\n",
        "  print('Respuesta de ref: ', ref[1]['respuesta'])\n",
        "  print('Respuesta RAG chain: ', rag_chain.invoke(ref[0]['pregunta']))\n",
        "  print('---------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las respuestas entregadas se acercan a lo esperado, dando cuenta que el modelo es capaz de entender lo que se le pregunta y buscar la mejor respuesta a partir de los documentos entregados."
      ],
      "metadata": {
        "id": "_EL-4FDqyNTe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8d5zTMHoUgF"
      },
      "source": [
        "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
        "\n",
        "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
        "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
        "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
        "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "retriever_chain = retriever | format_docs # chain\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(rag_template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever_chain, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt | llm | StrOutputParser() )\n",
        "\n",
        "for i, ref in enumerate(referencia):\n",
        "  print('Pregunta: ', ref[0]['pregunta'])\n",
        "  print('Respuesta de ref: ', ref[1]['respuesta'])\n",
        "  print('Respuesta RAG chain: ', rag_chain.invoke(ref[0]['pregunta']))\n",
        "  print('---------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "CI2r2huOaO6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDh_QgeXLGHc"
      },
      "outputs": [],
      "source": [
        "# Chunck de mayor tamaño\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "retriever_chain = retriever | format_docs # chain\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(rag_template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever_chain, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt | llm | StrOutputParser() )\n",
        "\n",
        "for i, ref in enumerate(referencia):\n",
        "  print('Pregunta: ', ref[0]['pregunta'])\n",
        "  print('Respuesta de ref: ', ref[1]['respuesta'])\n",
        "  print('Respuesta RAG chain: ', rag_chain.invoke(ref[0]['pregunta']))\n",
        "  print('---------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al aumentar el tamaño del chunk, se tiene más contexto por lo que tiene sentido que las respuestas entregadas usando chunk_size=1000, sean más completas."
      ],
      "metadata": {
        "id": "3eGk92MLZqCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cambio numero de docs a recuperar\n",
        "num_docs_recuperar = [1, 4]\n",
        "for k in num_docs_recuperar:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "\n",
        "    retriever_chain = retriever | format_docs # chain\n",
        "\n",
        "    rag_prompt = PromptTemplate.from_template(rag_template)\n",
        "\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever_chain, \"question\": RunnablePassthrough()}\n",
        "        | rag_prompt | llm | StrOutputParser() )\n",
        "\n",
        "    for i, ref in enumerate(referencia):\n",
        "      print('chuck recuperados :', k)\n",
        "      print('Pregunta: ', ref[0]['pregunta'])\n",
        "      print('Respuesta de ref: ', ref[1]['respuesta'])\n",
        "      print('Respuesta RAG chain: ', rag_chain.invoke(ref[0]['pregunta']))\n",
        "      print('---------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "fjtElim7P3rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tal como ocurrió cuando se aumentó el tamaño de los chunks, al aumentar el número de documentos por recuperar también mejoran las respuestas entregadas."
      ],
      "metadata": {
        "id": "QkcMgsz-ec7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tipo de busqueda MMR\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
        "\n",
        "retriever_chain = retriever | format_docs # chain\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(rag_template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever_chain, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt | llm | StrOutputParser() )\n",
        "\n",
        "for i, ref in enumerate(referencia):\n",
        "  print('Tipo de búsqueda :', 'mmr')\n",
        "  print('Pregunta: ', ref[0]['pregunta'])\n",
        "  print('Respuesta de ref: ', ref[1]['respuesta'])\n",
        "  print('Respuesta RAG chain: ', rag_chain.invoke(ref[0]['pregunta']))\n",
        "  print('---------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "D16QohJsQoz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al buscar por Maximal marginal Relevance (MMR), se obtienen respuestas más nutridas y diversas respecto a las entregadas mediante la búsqueda por similitud."
      ],
      "metadata": {
        "id": "-iXhNGIqTmGy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENJiPPM0giX8"
      },
      "source": [
        "### **2.2 Agentes (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V47l7Mjfrk0N"
      },
      "source": [
        "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6SLKwcWr0AG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "# inicializamos tool\n",
        "search_tavily = TavilySearchResults(max_results = 1, include_raw_content=True)\n",
        "tools = [search_tavily]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_tavily.invoke('qué frutas tienen alto contenido de fibra?')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q7nG2E1uUrPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SonB1A-9rtRq"
      },
      "source": [
        "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
        "\n",
        "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wikipedia"
      ],
      "metadata": {
        "id": "DTVliBeU28WX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehJJpoqsr26-"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "api_wrapper = WikipediaAPIWrapper(top_k_results=1,\n",
        "                                  doc_content_chars_max=600,\n",
        "                                  lang='es' # respuestas en español\n",
        "                                  )\n",
        "search_wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "\n",
        "tools += [search_wiki]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_wiki.invoke('qué frutas tienen alto contenido de fibra?')"
      ],
      "metadata": {
        "id": "sVbKFiDB5QJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUIMdX6r0ne"
      },
      "source": [
        "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
        "\n",
        "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD1_n0wrsDI5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain import hub\n",
        "\n",
        "\n",
        "react_prompt = hub.pull(\"hwchase17/react\") + PromptTemplate.from_template(\n",
        "                                                \"\"\"\n",
        "                                                Responde utilizando search_wiki si la pregunta puede ser\n",
        "                                                respondida mediante búsquedas en Wikipedia, sino utiliza\n",
        "                                                search_tavily.\n",
        "                                                No respondas, ni inventes respuestas, en caso de que la\n",
        "                                                pregunta no pueda ser respondida con ninguna tool.\n",
        "                                                Responde en español.\n",
        "                                                \"\"\")\n",
        "\n",
        "\n",
        "agente = create_react_agent(llm, tools, react_prompt) # primero inicializamos el agente ReAct\n",
        "agente_executor = AgentExecutor(agent=agente, tools=tools,\n",
        "                                verbose=True,\n",
        "                                return_intermediate_steps=True,\n",
        "                                max_iterations=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKV0JxK3r-XG"
      },
      "source": [
        "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En caso de que se esté buscando una respuesta, lo ideal es usar la tool de Tavily que busca, filtra y extrae información desde distintas fuentes en línea. La tool de Wikipedia, busca información solo a partir  de páginas de wikipedia, por lo que podría estar desactualizada o con errores."
      ],
      "metadata": {
        "id": "6mfZKeTGglyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agente_executor.invoke({'input':'qué pasó con Cathy Barriga?'})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LoFOn8TIWMev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agente_executor.invoke({'input':'últimos albums de Jin de BTS'})"
      ],
      "metadata": {
        "id": "zY__1iO61w82",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agente_executor.invoke({'input':'actual presidente de Chile'})"
      ],
      "metadata": {
        "id": "dgB-39KecjQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZbDTYiogquv"
      },
      "source": [
        "### **2.3 Multi Agente (1.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
        "\" width=\"450\">\n",
        "</p>\n",
        "\n",
        "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-iUfH0WvI6m"
      },
      "source": [
        "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
        "\n",
        "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw1cfTtvv1AZ"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def rag_solution(x: str) -> str:\n",
        "  \"\"\" Responde la pregunta 'x' con la solucion RAG, si esta está relacionada\n",
        "      con nutrición, comida y/o salud.\n",
        "    \"\"\"\n",
        "  return rag_chain.invoke(x)\n",
        "\n",
        "@tool\n",
        "def agent_tool(x:str) -> str:\n",
        "  \"\"\" Responde la pregunta 'x' con este agente si esta puede ser respondida\n",
        "      realizando búsquedas en páginas web o mediante wikipedia.\n",
        "  \"\"\"\n",
        "  return agente_executor.invoke({'input':x})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYNjT_0vPCg"
      },
      "source": [
        "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
        "\n",
        "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv2ZY0BAv1RD"
      },
      "outputs": [],
      "source": [
        "supervisor_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Responde a las preguntas de los usuarios de la mejor manera posible utilizando\n",
        "    las siguientes tools:\n",
        "    {tools}\n",
        "    Tú único rol es redireccionar la pregunta del usuario utilizando las tools\n",
        "    entregadas, dependiendo de si la pregunta es sobre nutrición y enfermedades\n",
        "    relacionadas con esta o realizar búsquedas en la web.\n",
        "    Recuerda ser amable y cordial en tu respuesta.\n",
        "    Restringete a utilizar solo los agentes dados, no inventes respuestas.\n",
        "\n",
        "    Pregunta: {question}\n",
        "    tool_names: {tools_name}\n",
        "    Respuesta cordial:\"\"\"\n",
        ")\n",
        "# agent_scratchpad = \"\"\n",
        "tools_supervisor = [rag_solution, agent_tool]\n",
        "tools_name = [rag_solution.name, agent_tool.name]\n",
        "supervisor = create_react_agent(llm,\n",
        "                                tools_supervisor,\n",
        "                                prompt=react_prompt)\n",
        "supervisor_executor = AgentExecutor(agent=supervisor,\n",
        "                                    tools=tools_supervisor,\n",
        "                                    verbose=True,\n",
        "                                    # return_intermediate_steps=True,\n",
        "                                    max_iterations=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3zWlvyvY7K"
      },
      "source": [
        "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREGUNTA 2.1.4\n",
        "supervisor_executor.invoke({'input': 'que alimentos debe tener la dieta de una persona con diabetes?'})"
      ],
      "metadata": {
        "id": "tsEmFeCGiW2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta respuesta parece estar más procesada que las anteriores, entregando información relevante de manera ordenada."
      ],
      "metadata": {
        "id": "G2pem9NJoX9k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_1t0zkgv1qW"
      },
      "outputs": [],
      "source": [
        "# PREGUNTA 2.2.4\n",
        "supervisor_executor.invoke({'input':'qué pasó con Cathy Barriga?'})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que esta respuesta se responde utilizando la tool de wikipedia, era esperable que la respuesta no cambiara ya que se está extrayendo información de la misma página."
      ],
      "metadata": {
        "id": "2zmUnuZKp7J9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb8bdAmYvgwn"
      },
      "source": [
        "#### **2.3.4 Análisis (0.25 puntos)**\n",
        "\n",
        "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAUlJxqoLK5r"
      },
      "source": [
        "El agente supervisor, a diferencia de la solución Router, asigna un agente para responder la pregunta entregada. Mientras que la solución router, toma la pregunta y la clasifica según el tema/objetivo de esta, pero necesita de otro agente para redireccionar esta pregunta, de manera tal que pueda ser respondida.\n",
        "\n",
        "- Ventaja: Más rápida y directa.\n",
        "- Desventaja: Dado que depende mucho del prompt entregado, la clasificación/asignación de un agente podría ser difícil para el modelo, generando errores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JWVSuWiZ8Mj"
      },
      "source": [
        "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
        "\n",
        "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
        "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
        "- Pregunta 2: \"Cual es mi nombre?\"\n",
        "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
        "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
        "\n",
        "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
        "\n",
        "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Y7tIPJLPfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFc3jBT5g0kT"
      },
      "source": [
        "### **2.5 Despliegue (0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
        "\n",
        "Primero instalamos la librería:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TsvnCPbkIA"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBztEUovKsF"
      },
      "source": [
        "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3KedQSvg1-n"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "def agent_response(message, history):\n",
        "  '''\n",
        "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
        "  '''\n",
        "  # get chatbot response\n",
        "  response = ... # rellenar con la respuesta de su chat\n",
        "\n",
        "  # assert\n",
        "  assert type(response) == str, \"output de route_question debe ser string\"\n",
        "\n",
        "  # \"streaming\" response\n",
        "  for i in range(len(response)):\n",
        "    time.sleep(0.015)\n",
        "    yield response[: i+1]\n",
        "\n",
        "gr.ChatInterface(\n",
        "    agent_response,\n",
        "    type=\"messages\",\n",
        "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
        "    description=\"Hola! Soy un chatbot muy útil :)\", # también la descripción\n",
        "    theme=\"soft\",\n",
        "    ).launch(\n",
        "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
        "        debug = False,\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
