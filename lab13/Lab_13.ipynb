{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl0JoW4Eodvi"
   },
   "source": [
    "# **Laboratorio 13: 游눧 Airflow 游눧**\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos</strong></center>\n",
    "\n",
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti치n Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicol치s Ojeda, Melanie Pe침a, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3ypG7Fsodvj"
   },
   "source": [
    "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados\n",
    "\n",
    "- Nombre de alumno 1: Alonso Uribe\n",
    "- Nombre de alumno 2: Carolina Nu침ez\n",
    "\n",
    "### **Link de repositorio de GitHub:** [Repositorio游눹](https://github.com/carinunez/Labs_MDS/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_P7PCPTodvk"
   },
   "source": [
    "## Temas a tratar\n",
    "\n",
    "- Construcci칩n de pipelines productivos usando `Airflow`.\n",
    "\n",
    "\n",
    "## Reglas:\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer material del curso que estimen conveniente.\n",
    "\n",
    "### Objetivos principales del laboratorio\n",
    "\n",
    "- Reconocer los componentes pricipales de `Airflow` y su funcionamiento.\n",
    "- Poner en pr치ctica la construcci칩n de pipelines de `Airflow`.\n",
    "- Automatizar procesos t칤picos de un proyecto de ciencia de datos mediante `Airflow` y `Docker`.\n",
    "\n",
    "El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsfK981Uodvk"
   },
   "source": [
    "# **Introducci칩n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ilM8YDjodvk"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.gifer.com/SUFL.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zrLPQNBodvk"
   },
   "source": [
    "Nico, un estudiante del Mag칤ster en Ciencia de Datos, se encuentra en la etapa final de sus estudios. Por un lado, est치 muy contento por haber llegado tan lejos, pero por otro, no puede evitar sentirse inquieto. Desde que ingres칩 a la universidad, una pregunta lo ha perseguido: 쯤u칠 tan probable es que pueda ser seleccionado en los lugares donde env칤e postulaciones para puestos de trabajo?\n",
    "\n",
    "Esta duda lo mantiene en constante reflexi칩n, especialmente porque sabe que el mercado laboral en Ciencia de Datos es competitivo y exige habilidades no solo t칠cnicas, sino tambi칠n estrat칠gicas para destacar. Sin embargo, Nico actualmente est치 completamente enfocado en terminar su tesis de mag칤ster y ha tenido que postergar cualquier preparaci칩n espec칤fica para enfrentar el desaf칤o de las postulaciones laborales.\n",
    "\n",
    "Al ver el avance y las habilidades que usted ha demostrado en el curso, Nico decidi칩 proponerle un desaf칤o que le permitir치 disminuir la incertidumbre sobre su futuro laboral. Inspirado en sus conocimientos, 칠l recolect칩 un conjunto de datos que contiene informaci칩n sobre diversos factores que influyen en las decisiones de contrataci칩n de empresas al seleccionar entre sus postulantes. Este set de datos incluye los siguientes atributos:\n",
    "\n",
    "- Age: Edad del candidato\n",
    "- Gender: Genero del candidato. Male (0), Female (1).\n",
    "- EducationLevel: Mayor nivel educacional alcanzado por el candidato. Licenciatura Tipo 1 (1), Licenciatura Tipo 2 (2), Maestr칤a (3), PhD. (4).\n",
    "- ExperienceYears: A침os de experiencia profesional.\n",
    "- PreviousCompanies: Numero de compa침칤as donde el candidato ha trabajado anteriormente.\n",
    "- DistanceFromCompany: Distancia en kilometros entre la residencia del candidato y la compa침칤a donde postula.\n",
    "- InterviewScore: Puntaje obtenido en la entrevista por el candidato entre 0 a 100.\n",
    "- SkillScore: Puntaje obtenido en evaluaci칩n de habilidades t칠cnicas por el candidato, entre 0 a 100.\n",
    "- PersonalityScore: Puntaje obtenido en pruebas de personalidad del candidato, entre 0 a 100.\n",
    "- RecruitmentStrategy: Estrategia del equipo de reclutamiento. Agresiva (1), Moderada (2), Conservadora (3).\n",
    "\n",
    "Variable a predecir:\n",
    "- HiringDecision: Resultado de la postulaci칩n. No contratado (0), Contratado (1).\n",
    "\n",
    "Su objetivo ser치 ayudar a Nico a desarrollar un modelo que le permita predecir, basado en estos factores, si un postulante ser치 contratado o no. Esta herramienta no solo le dar치 a Nico mayor claridad sobre el impacto de ciertos atributos en la decisi칩n final de contrataci칩n, sino que tambi칠n le permitir치 aplicar sus conocimientos de Ciencia de Datos para resolver una pregunta que a muchos estudiantes como 칠l les inquieta.\n",
    "\n",
    "Como estudiante del curso Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos, deber치 demostrar sus capacidades para preprocesar, analizar y modelar datos, brind치ndole a Nico una soluci칩n robusta y bien fundamentada para su problem치tica.\n",
    "\n",
    "`Nota:` El siguiente [enlace](https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data/data) contiene el set de datos original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yeh268atodvl"
   },
   "source": [
    "# **1. Pipeline de Predicci칩n Lineal** (30 Puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmB1LTWnodvl"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://c.tenor.com/WvHhQt2UpuAAAAAd/wolf-of-wall-street.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF1bTY0Modvl"
   },
   "source": [
    "En esta secci칩n buscaremos desplegar un producto utilizando un modelo de clasificaci칩n `Random Forest` para determinar si una persona ser치 contratada o no en un proceso de selecci칩n. Para ello, comenzaremos preparando un pipeline lineal mediante `Airflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7MllF4fodvl"
   },
   "source": [
    "## **1.1 Preparando el Pipeline** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1JxaZgModvl"
   },
   "source": [
    "**Primero, aseg칰rese de tener creada las carpetas `dags`, `plugins` y `logs`**.\n",
    "\n",
    "Comenzamos preparando un archivo llamado `hiring_functions.py`, el cual guardar치 en la carpeta `dags` y debe contener lo siguiente:\n",
    "\n",
    "1. (3 puntos) Una funci칩n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci칩n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
    "  - raw\n",
    "  - splits\n",
    "  - models\n",
    "\n",
    "  `Hint`: Puede hacer uso de kwargs para obtener la fecha de ejecuci칩n mediante el DAG. El siguiente [Enlace](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html) le puede ser 칰til.\n",
    "\n",
    "2. (3 puntos) Una funci칩n llamada `split_data()` que lea el archivo `data_1.csv` de la carepta `raw` y a partir de este, aplique un *hold out*, generando un dataset de entrenamiento y uno de prueba. Luego debe guardar estos nuevos conjuntos de datos en la carpeta `splits`. `Nota:` Utilice un 20% para el conjunto de prueba, mantenga la proporci칩n original en la variable objetivo y fije una semilla.\n",
    "\n",
    "3. (8 puntos) Cree una funci칩n llamada `preprocess_and_train()` que:\n",
    "  - Lea los set de entrenamiento y prueba de la carpeta `splits`.\n",
    "  - Cree y aplique un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes. Puede apoyarse del archivo `data_1_report.html` para justificar cualquier paso del preprocesamiento.\n",
    "  \n",
    "  - A침ada una etapa de entrenamiento utilizando el modelo `RandomForest`.\n",
    "  \n",
    "  Esta funci칩n **debe crear un archivo `joblib` (an치logo a `pickle`) con el pipeline entrenado** en la carepta `models`, adem치s debe **imprimir** el accuracy en el conjunto de prueba y el f1-score de la clase positiva (contratado).\n",
    "  \n",
    "3. (1 punto) Incorpore la funci칩n `gradio_interface` en su script, modificando la ruta de acceso a su modelo, de forma que pueda leerlo desde la carepta `models`. Puede realizar modificacioneds adicionales en caso de ser necesario.\n",
    "\n",
    "`NOTA:` Se permite la creaci칩n de funciones auxiliares si lo estiman conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1732807326416,
     "user": {
      "displayName": "Eduardo Andr칠s Moya Briones",
      "userId": "15114984037692151374"
     },
     "user_tz": 180
    },
    "id": "ze9Iotloodvl"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import logging\n",
    "import os\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import gradio as gr\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# /usr/bin/bash -c curl -o ./dags/2024-12-05/raw/data_1.csv https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\n",
    "# /usr/bin/bash -c curl -o dags/2024-12-04/raw/ https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\n",
    "def create_folders(**kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "\n",
    "    os.makedirs(join('.', 'dags', date), exist_ok=True)\n",
    "    os.makedirs(join('.', 'dags', date, \"raw\"), exist_ok=True)\n",
    "    os.makedirs(join('.', 'dags', date, \"splits\"), exist_ok=True)\n",
    "    os.makedirs(join('.', 'dags', date, \"models\"), exist_ok=True)\n",
    "\n",
    "def split_data(**kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "\n",
    "    df = pd.read_csv(join('.', 'dags', date, 'raw', 'data_1.csv'))\n",
    "    X = df.drop(columns=['HiringDecision'])\n",
    "    y = df['HiringDecision']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=29)\n",
    "\n",
    "    # exporta los datasets sin index\n",
    "    X_train.to_csv(join('.', 'dags', date, 'splits', 'X_train.csv'),index=False)\n",
    "    X_test.to_csv(join('.', 'dags', date, 'splits', 'X_test.csv'),  index=False)\n",
    "    y_train.to_csv(join('.', 'dags', date, 'splits', 'y_train.csv'), index=False)\n",
    "    y_test.to_csv(join('.', 'dags', date, 'splits', 'y_test.csv'), index=False)\n",
    "\n",
    "def preprocess_and_train(**kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "    X_train = pd.read_csv(join('.', 'dags', date, 'splits', 'X_train.csv'))\n",
    "    y_train = pd.read_csv(join('.', 'dags', date, 'splits', 'y_train.csv'))\n",
    "\n",
    "    X_test = pd.read_csv(join('.', 'dags', date, 'splits', 'X_test.csv'))\n",
    "    y_test = pd.read_csv(join('.', 'dags', date, 'splits', 'y_test.csv'))\n",
    "\n",
    "    clasico = ColumnTransformer([\n",
    "        ('minmas', MinMaxScaler(), X_train.select_dtypes(include='number').columns),\n",
    "        ('nada', 'passthrough', X_train.select_dtypes(include='category').columns)\n",
    "    ],\n",
    "    verbose_feature_names_out=True)\n",
    "    clasico.set_output(transform='pandas')\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=29)\n",
    "\n",
    "    pipe_clasica = Pipeline([\n",
    "        ('col_trans', clasico),\n",
    "        ('random', rf)\n",
    "    ])\n",
    "    rf_pipe = pipe_clasica\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "    y_pred = rf_pipe.predict(X_test)\n",
    "    \n",
    "    logging.info('y test:', y_test)\n",
    "    logging.info('Predicted:', y_pred)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    logging.info(\"Accuracy:\", accuracy) \n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    logging.info(\"F1-Score:\", f1 )\n",
    "\n",
    "    with open(join('.', 'dags', date, 'models','randomforest.zlib'), 'wb') as randomfile:\n",
    "        joblib.dump(rf_pipe, randomfile)\n",
    "\n",
    "def predict(file, model_path):\n",
    "    pipeline = joblib.load(model_path)\n",
    "    input_data = pd.read_json(file)\n",
    "    predictions = pipeline.predict(input_data)\n",
    "    print(f'La prediccion es: {predictions}')\n",
    "    labels = [\"No contratado\" if pred == 0 else \"Contratado\" for pred in predictions]\n",
    "\n",
    "    return {'Predicci칩n': labels[0]}\n",
    "\n",
    "def gradio_interface(**kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "    model_path = join('.', 'dags', date, 'models', 'randomforest.zlib')\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn = lambda file: predict(file, model_path),\n",
    "        inputs = gr.File(label=\"Sube un archivo JSON\"),\n",
    "        outputs = \"json\",\n",
    "        title = \"Hiring Decision Prediction\",\n",
    "        description = \"Sube un archivo JSON con las caracter칤sticas de entrada para predecir si Nico ser치 contratado o no.\"\n",
    "    )\n",
    "    \n",
    "    interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_RCVPnUodvm"
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(file,model_path):\n",
    "\n",
    "    pipeline = joblib.load(model_path)\n",
    "    input_data = pd.read_json(file)\n",
    "    predictions = pipeline.predict(input_data)\n",
    "    print(f'La prediccion es: {predictions}')\n",
    "    labels = [\"No contratado\" if pred == 0 else \"Contratado\" for pred in predictions]\n",
    "\n",
    "    return {'Predicci칩n': labels[0]}\n",
    "\n",
    "\n",
    "def gradio_interface():\n",
    "\n",
    "    model_path= ... #Completar con la ruta del modelo entrenado\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn=lambda file: predict(file, model_path),\n",
    "        inputs=gr.File(label=\"Sube un archivo JSON\"),\n",
    "        outputs=\"json\",\n",
    "        title=\"Hiring Decision Prediction\",\n",
    "        description=\"Sube un archivo JSON con las caracter칤sticas de entrada para predecir si Nico ser치 contratado o no.\"\n",
    "    )\n",
    "    interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTKOj1hfodvm"
   },
   "source": [
    "## **1.2 Creando Nuestro DAG** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkEZcEh4odvm"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExMzNjd3hxOWIzZjhwZDc5NnJwZzZodnNrbWI5cGtjY2VwZjI0eDdnOSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Dh5q0sShxgp13DwrvG/giphy.webp\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-MTaxTgodvm"
   },
   "source": [
    "Con las funciones del pipeline ya creadas, ahora vamos a proceder a crear un Directed Acyclic Graph (DAG). Para ello, se le pide lo siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-yUak2Rodvm"
   },
   "source": [
    "- (10 puntos) Cree un segundo archivo llamado `dag_lineal.py` y guardelo en la carpeta dags. Este script debe seguir la siguiente estructura (Ver imagen de referencia):\n",
    "\n",
    "    0. Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, ejecuci칩n manual y **sin backfill**. Asigne un `dag_id` que pueda reconocer facilmente, como `hiring_lineal`, etc.\n",
    "    1. Debe comenzar con un marcador de posici칩n que indique el inicio del pipeline.\n",
    "    2. Cree una carpeta correspondiente a la ejecuci칩n del pipeline y cree las subcarpetas `raw`, `splits` y `models` mediante la funci칩n `create_folders()`.\n",
    "    3. Debe descargar el archivo `data_1.csv` del siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv). Debe guardar el archivo en la carpeta raw de la ejecuci칩n correspondiente.`Hint:` Le puede ser 칰til el comando `curl -o <path de guardado> <enlace con los datos>`.\n",
    "    4. Debe aplicar un hold out mediante la funci칩n `split_data()` de su archivo creado en la subsecci칩n anterior.\n",
    "    5. Debe aplicar el preprocesamiento y el entrenamiento del modelo mediante la funci칩n `preprocess_and_train()`.\n",
    "    6. Finalmente, debe montar una interfaz en gradio donde pueda cargar un archivo ``json``.\n",
    "\n",
    "\n",
    "- (3 puntos) Cree un `DockerFile` para montar un contenedor que contenga Airflow. Adicionalmente, cree una carpeta llamada dags donde guardar치 el script.py creado anteriormente.\n",
    "\n",
    "    `Nota:` Para la imagen, se recomienda utilizar python 3.10-slim. Adicionalmente, puede instalar `curl` mediante la siguiente linea de c칩digo: `RUN apt-get update && apt-get install -y curl`.\n",
    "\n",
    "- Construya el contenedor en Docker y acceda a la aplicaci칩n web de Airflow mediante el siguiente [enlace](http://localhost:8080/). Inicie sesi칩n, acceda al DAG creado y ejecute de forma manual su pipeline.\n",
    "\n",
    "- (2 puntos) Acceda a la URL p칰blica de Gradio e ingrese el archivo `nico_data.json` a su modelo. 쯈ue predicci칩n entreg칩 el modelo para Nico? Adjunte im치genes de su resultado. `Hint:` Puede acceder a los `logs` para obtener los prints y la URL p칰blica.\n",
    "\n",
    "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci칩n `ds`.\n",
    "\n",
    "**Para esta secci칩n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser치n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im치genes de apoyo, como screenshots.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiMTgQfJpuIv"
   },
   "source": [
    "DAG de referencia:\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://drive.google.com/uc?id=1iwDgECZfFeWq1dl433tMa6_3CNF9cn1L\" width=\"1200\">\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckzDqsF4odvn"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os.path import join\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from hiring_functions import create_folders, split_data, preprocess_and_train, predict, gradio_interface\n",
    "\n",
    "\n",
    "start_date = datetime(2024, 10, 1)\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,  # Evita backfill\n",
    "    'start_date': start_date,\n",
    "    'retries': 0,  # No intentos adicionales\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='hiring_lineal',  # Cambiar por un ID que desees\n",
    "    default_args=default_args,\n",
    "    schedule_interval=None,  # Ejecuci칩n manual\n",
    "    catchup=False,  # No realizar backfill\n",
    "    description='DAG lineal para contrataci칩n',\n",
    "    tags=['example', 'lineal', 'contrataci칩n']  # Opcional, etiquetas\n",
    ") as dag:\n",
    "\n",
    "    # Definimos tareas dummy como placeholders\n",
    "    tarea_1 = EmptyOperator(task_id='inicio')\n",
    "    \n",
    "    tarea_2 = PythonOperator(\n",
    "        task_id='crear_carpetas',\n",
    "        python_callable=create_folders,\n",
    "        provide_context=True  # Proveer contexto para acceso a ds (execution_date)\n",
    "    )\n",
    "    tarea_3 = BashOperator(\n",
    "        task_id='raw_files',\n",
    "        bash_command=\"curl -o $AIRFLOW_HOME/dags/{{ execution_date.strftime('%Y-%m-%d') }}/raw/data_1.csv https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\",\n",
    "        dag=dag\n",
    "    )\n",
    "    tarea_4 = PythonOperator(\n",
    "        task_id='train_test_split',\n",
    "        python_callable=split_data,\n",
    "        provide_context=True\n",
    "    )\n",
    "    tarea_5 = PythonOperator(\n",
    "        task_id='preprocess_train',\n",
    "        python_callable=preprocess_and_train,\n",
    "        provide_context=True\n",
    "    )\n",
    "    tarea_6 = PythonOperator(\n",
    "        task_id='to_gradio_app',\n",
    "        python_callable=gradio_interface,\n",
    "        provide_context=True\n",
    "    )\n",
    "    fin = EmptyOperator(task_id='fin')\n",
    "\n",
    "    # Definimos la estructura lineal de las tareas\n",
    "    tarea_1 >> tarea_2 >> tarea_3 >> tarea_4 >> tarea_5 >> tarea_6 >> fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](parte1/gradio_pred.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqBlHcBQpXJb"
   },
   "source": [
    "# **2. Paralelizando el Pipeline** (30 puntos)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/vDv7mn58skcAAAAM/clapping.gif\" width=\"300\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoQaVOeiqO_R"
   },
   "source": [
    "Al ver los resultados obtenidos, Nico queda muy contento con el clasificador. Sin embargo, le aparecen algunas dudas respecto al funcionamiento del pipeline. Primero le comenta que es posible que en un futuro tenga nuevos datos que podr칤an ser 칰tiles para realizar nuevos entrenamientos, por lo que ser칤a ideal si este pipeline se fuera ejecutando de forma peri칩dica y no de forma manual. Adem치s, Nico le menciona que le gustar칤a explorar el desempe침o de otros modelos adem치s de `Random Forest`, de forma que el pipeline seleccione de forma autom치tica el modelo con mejor desempe침o para luego hacer la predicci칩n de Nico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mGPMg0ur-wR"
   },
   "source": [
    "## **2.1 Preparando un Nuevo Pipeline** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpU81VCRr-Hr"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.makeagif.com/media/7-07-2015/oH6WRw.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KcXuS6bsZAw"
   },
   "source": [
    "De acuerdo a lo que le coment칩 Nico, usted decide crear un nuevo script con las funciones que utilizar치 su pipeline. Por ende, dentro de la carpeta `dags`, usted crear치 el archivo `hiring_dynamic_functions.py` el cual debe contener:\n",
    "\n",
    "1. (2 puntos) Una funci칩n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci칩n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
    "  - raw\n",
    "  - preprocessed\n",
    "  - splits\n",
    "  - models\n",
    "2. (2 puntos) Una funci칩n llamada `load_ands_merge()` que lea desde la carpeta `raw` los archivos `data_1.csv`y `data_2.csv` en caso de estar disponible. Luego concatene estos y genere un nuevo archivo resultante, guard치ndolo en la carpeta `preprocessed`.\n",
    "\n",
    "3. (2 puntos) Una funci칩n llamada `split_data()` que lea la data guardada en la carpeta `preprocessed` y realice un hold out sobre esta data. Esta funci칩n debe crear un conjunto de entrenamiento y uno de prueba. Mantenga una semilla y 20% para el conjunto de prueba. Guarde los conjuntos resultantes en la carpeta `splits`.\n",
    "\n",
    "4. (6 puntos) Una funci칩n llamada `train_model()` que reciba un modelo de clasificaci칩n.\n",
    "    - La funci칩n debe comenzar leyendo el conjunto de entrenamiento desde la carpeta `spits`.\n",
    "    - Esta debe crear y aplicar un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes.\n",
    "    - A침ada una etapa de entrenamiento utilizando un modelo que ingrese a la funci칩n.\n",
    "  \n",
    "  Esta funci칩n **debe crear un archivo joblib con el pipeline entrenado**. Guarde el modelo con un nombre que le permita una facil identificaci칩n dentro de la carpeta `models`.\n",
    "\n",
    "5. (3 puntos) Una funci칩n llamada `evaluate_models()` que reciba sus modelos entrenados desde la carpeta `models`, eval칰e su desempe침o mediante `accuracy` en el conjunto de prueba y seleccione el mejor modelo obtenido. Luego guarde el mejor modelo como archivo `.joblib`. Su funci칩n debe imprimir el nombre del modelo seleccionado y el accuracy obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnX61hxjW9rI"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import logging\n",
    "import os\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import gradio as gr\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def create_folders(**kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "    print(type(date), date)\n",
    "\n",
    "    os.makedirs(join('.', 'dags', date), exist_ok=True)\n",
    "    os.makedirs(join('.', 'dags', date, \"raw\"), exist_ok=True)\n",
    "    os.makedirs(join('.', 'dags', date, \"preprocessed\"), exist_ok=True)\n",
    "    os.makedirs(join('.', 'dags', date, \"splits\"), exist_ok=True)\n",
    "    os.makedirs(join('.', 'dags', date, \"models\"), exist_ok=True)\n",
    "\n",
    "def load_and_merge(**kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "    \n",
    "    df1 = pd.read_csv(join('.', 'dags', date, 'raw', 'data_1.csv'))\n",
    "    if os.path.exists(join('.', 'dags', date, 'raw', 'data_2.csv')):\n",
    "        df2 = pd.read_csv(join('.', 'dags', date, 'raw', 'data_2.csv'))\n",
    "        data = pd.concat([df1, df2], axis=0)\n",
    "    else:\n",
    "        data = df1.copy()\n",
    "\n",
    "    data.to_csv(join('.', 'dags', date, 'preprocessed', 'data.csv'))\n",
    "\n",
    "def split_data(**kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "\n",
    "    df = pd.read_csv(join('.', 'dags', date, 'preprocessed', 'data.csv'))\n",
    "    X = df.drop(columns=['HiringDecision'])\n",
    "    y = df['HiringDecision']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=29)\n",
    "\n",
    "    X_train.to_csv(join('.', 'dags', date, 'splits', 'X_train.csv'), index=False)\n",
    "    X_test.to_csv(join('.', 'dags', date, 'splits', 'X_test.csv'), index=False)\n",
    "    y_train.to_csv(join('.', 'dags', date, 'splits', 'y_train.csv'), index=False)\n",
    "    y_test.to_csv(join('.', 'dags', date, 'splits', 'y_test.csv'), index=False)\n",
    "\n",
    "def train_model(model, **kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "    X_train = pd.read_csv(join('.', 'dags', date, 'splits', 'X_train.csv'))\n",
    "    y_train = pd.read_csv(join('.', 'dags', date, 'splits', 'y_train.csv'))\n",
    "\n",
    "    clasico = ColumnTransformer([\n",
    "        ('minmas', MinMaxScaler(), X_train.select_dtypes(include='number').columns),\n",
    "        ('nada', 'passthrough', X_train.select_dtypes(include='category').columns)\n",
    "    ],\n",
    "    verbose_feature_names_out=True)\n",
    "    clasico.set_output(transform='pandas')\n",
    "\n",
    "    model_pipe = Pipeline([\n",
    "        ('col_trans', clasico),\n",
    "        ('random', model)\n",
    "    ])\n",
    "\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "\n",
    "    with open(join('.', 'dags', date, 'models', f'{model.__class__.__name__}.zlib'), 'wb') as modelfile:\n",
    "        joblib.dump(model_pipe, modelfile)\n",
    "\n",
    "def evaluate_models(**kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "\n",
    "    max_acc = 0.\n",
    "    best_model = None\n",
    "    for file in os.listdir(join('.', 'dags', date, 'models')):\n",
    "        model_name = file.split('.')[0]\n",
    "        with open(join('.', 'dags', date, 'models', str(file)), 'rb') as modelfile:\n",
    "            model_pipe = joblib.load(modelfile)\n",
    "\n",
    "        X_test = pd.read_csv(join('.', 'dags', date, 'splits', 'X_test.csv'))\n",
    "        y_test = pd.read_csv(join('.', 'dags', date, 'splits', 'y_test.csv'))\n",
    "        y_pred = model_pipe.predict(X_test)\n",
    "        acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "        logging.info(f\"Model: {model_name}, Accuracy: {acc}\")\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            best_model_name = model_name\n",
    "            best_model = model_pipe\n",
    "\n",
    "    with open(join('.', 'dags', date, 'models', 'besto_'+str(file)), 'wb') as modelfile:\n",
    "            model_pipe = joblib.dump(best_model, modelfile)\n",
    "            \n",
    "    logging.info(f\"Besto model: {best_model_name}, Accuracy: {max_acc}\")\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUYkXWcZJz3b"
   },
   "source": [
    "## **2.2 Componiendo un nuevo DAG** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak7uL9YXJ6Xj"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/977552ab-0b55-4118-9948-06f6386474da_text.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbE6mu20LfWd"
   },
   "source": [
    "Con las nuevas funciones, vamos a crear nuestro nuevo DAG. Para ello, cree un nuevo script en la carpeta `dags`, llamandolo `dag_dynamic.py`. Este script debe contener la siguiente estructura:\n",
    "\n",
    "1. (1 punto) Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, el cual se debe ejecutar el d칤a 5 de cada mes a las 15:00 UTC. Utilice un `dag_id` interpretable para identificar f치cilmente. **Habilite el backfill** para que pueda ejecutar tareas programadas desde fechas pasadas.\n",
    "2. (1 punto) Comience con un marcador de posici칩n que indique el inicio del pipeline.\n",
    "3. (2 puntos) Cree una carpeta correspondiente a la ejecuci칩n del pipeline y cree las subcarpetas `raw`, `preprocessed`, `splits` y `models` mediante la funci칩n `create_folders()`.\n",
    "4. (2 puntos) Implemente un `Branching`que siga la siguiente l칩gica:\n",
    "  - Fechas previas al 1 de noviembre de 2024: Se descarga solo `data_1.csv`\n",
    "  - Desde el 1 de noviembre del 2024: descarga `data_1.csv` y `data_2.csv`.\n",
    "  En el siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv) puede descargar `data_2.csv`.\n",
    "5. (1 punto) Cree una tarea que concatene los datasets disponibles mediante la funci칩n `load_and_merge()`. Configure un `Trigger` para que la tarea se ejecute si encuentra disponible **como m칤nimo** uno de los archivos.\n",
    "6. (1 punto) Aplique el hold out al dataset mediante la funci칩n `split_data()`, obteniendo un conjunto de entrenamiento y uno de prueba.\n",
    "7. (2 puntos) Realice 3 entrenamientos en paralelo:\n",
    "  - Un modelo Random Forest.\n",
    "  - 2 modelos a elecci칩n.\n",
    "  Aseg칰rese de guardar sus modelos entrenados con nombres distintivos. Utilice su funci칩n `train_model()` para ello.\n",
    "8. (2 puntos)Mediante la funci칩n `evaluate_models()`, eval칰e los modelos entrenados, registrando el accuracy de cada modelo en el set de prueba. Luego debe imprimir el mejor modelo seleccionado y su respectiva m칠trica. Configure un `Trigger` para que la tarea se ejecute solamente si los 3 modelos fueron entrenados y guardados.\n",
    "\n",
    "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci칩n `ds`.\n",
    "\n",
    "Una vez creado el script, vuelva a construir el contenedor en Docker, acceda a la aplicaci칩n web de Airflow, ejecute su pipeline y muestre sus resultados. Adjunte im치genes que ayuden a mostrar el proceso y sus resultados.\n",
    "\n",
    "Adicionalmente, responda (1 c/u):\n",
    "\n",
    "1. 쮺ual es el accuracy de cada modelo en la ejecuci칩n de octubre? 쯉e obtienen los mismos resultados a partir de Noviembre?\n",
    "2. Analice como afect칩 el a침adir datos a sus modelos mediante el desempe침o del modelo y en costo computacional.\n",
    "3. Muestre el esquema de su DAG ejecutado en octubre y en noviembre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. - 68, 84, 93 % para Dummy, DecisionTree y RandomForest respectivamente, para Octubre. \n",
    "    - 69, 91, 91.6 % para Dummy, DecisionTree y RandomForest respectivamente, para Noviembre. \n",
    "2. Vemos que el accuracy para el mejor modelo: RandomForest decae con m치s datos. El desempe침o del model empeoro levente para este modelo en particular. Para los dem치s mejoro muy levemente. Los entrenamientos se demoraron un poco m치s para el flujo de Noviembre debido a la mayor cantidad de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](parte2/diagOct.jpg) ![alt text](parte2/diagNov.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](parte2/metricaOct.jpg) ![alt text](parte2/metricaNov.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Nota:` Para esta secci칩n no debe implementar la tarea en gradio, solamente se espera determinar el mejor modelo y comparar el desempe침o obtenido.\n",
    "\n",
    "**IMPORTANTE: Para esta secci칩n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser치n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im치genes de apoyo, como screenshots.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMgK2sKTYJji"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os.path import join\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from hiring_dynamic_functions import create_folders, load_and_merge, split_data, train_model, evaluate_models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def choose_branch(**kwargs):\n",
    "    date = kwargs.get('ds')\n",
    "    threshold_date = datetime.strptime('2024-11-01', \"%Y-%m-%d\")\n",
    "    date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    if date < threshold_date:\n",
    "        return 'dl_data_1'\n",
    "    else:\n",
    "        return ['dl_data_1', 'dl_data_2']\n",
    "    \n",
    "\n",
    "start_date = datetime(2024, 10, 1)\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    # 'depends_on_past': False,  # Evita backfill\n",
    "    'start_date': start_date,\n",
    "    'retries': 0,  # No intentos adicionales\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='hiring_dynamic',  # Cambiar por un ID que desees\n",
    "    default_args=default_args,\n",
    "    schedule_interval='0 15 5 * *',  \n",
    "    catchup=True,  # Realiza backfill\n",
    "    description='DAG lineal para contrataci칩n',\n",
    "    tags=['example', 'lineal', 'contrataci칩n']  # Opcional, etiquetas\n",
    ") as dag:\n",
    "\n",
    "    # Definimos tareas dummy como placeholders\n",
    "    tarea_1 = EmptyOperator(task_id='inicio')\n",
    "    \n",
    "    tarea_2 = PythonOperator(\n",
    "        task_id='crear_carpetas',\n",
    "        python_callable=create_folders,\n",
    "        provide_context=True  # Proveer contexto para acceso a ds (execution_date)\n",
    "    )\n",
    "    branch_1 = BranchPythonOperator(\n",
    "        task_id='branch_1',\n",
    "        python_callable=choose_branch, \n",
    "        provide_context=True, \n",
    "        dag=dag\n",
    "    )\n",
    "    dl_data_1 = BashOperator(\n",
    "        task_id='dl_data_1',\n",
    "        bash_command=\"curl -o $AIRFLOW_HOME/dags/{{ execution_date.strftime('%Y-%m-%d') }}/raw/data_1.csv https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\",\n",
    "        dag=dag\n",
    "    )\n",
    "    dl_data_2 = BashOperator(\n",
    "        task_id='dl_data_2',\n",
    "        bash_command=\"curl -o $AIRFLOW_HOME/dags/{{ execution_date.strftime('%Y-%m-%d') }}/raw/data_2.csv https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv\",\n",
    "        dag=dag\n",
    "    )\n",
    "    loadmerge = PythonOperator(\n",
    "        task_id='load_and_merge',\n",
    "        python_callable=load_and_merge,\n",
    "        provide_context=True,\n",
    "        trigger_rule='one_success'\n",
    "    )\n",
    "    split = PythonOperator(\n",
    "        task_id='split_data',\n",
    "        python_callable=split_data,\n",
    "        provide_context=True\n",
    "    )\n",
    "    def train_operator(model):\n",
    "        train_op = PythonOperator(\n",
    "            task_id=f'train_model_{model.__class__.__name__}',\n",
    "            python_callable=train_model,\n",
    "            provide_context=True,\n",
    "            op_kwargs={'model':model}\n",
    "        )\n",
    "        return train_op\n",
    "    \n",
    "    eval = PythonOperator(\n",
    "        task_id='eval_model',\n",
    "        python_callable=evaluate_models,\n",
    "        provide_context=True,\n",
    "        trigger_rule='all_success'\n",
    "    )\n",
    "    fin = EmptyOperator(task_id='fin')\n",
    "\n",
    "    # Definimos la estructura lineal de las tareas\n",
    "    tarea_1 >> tarea_2 >>  branch_1 # Tronco\n",
    "    branch_1 >> [dl_data_1, dl_data_2] >> loadmerge >> split\n",
    "    split >> [train_operator(RandomForestClassifier(random_state=29)), train_operator(DecisionTreeClassifier(random_state=29)), train_operator(DummyClassifier(random_state=29)) ] >> eval >> fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrmM65RIRrgm"
   },
   "source": [
    "# Conclusi칩n\n",
    "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por el foro de U-cursos o por correo.\n",
    "\n",
    "<center>\n",
    "<img src =\"https://media0.giphy.com/media/W12WAzuqod9VS/200w.gif?cid=6c09b952gekq3fm1no1ttwcvgm9oj3khbm6yxbe6qwnx3nad&ep=v1_gifs_search&rid=200w.gif&ct=g\" width = 400 />\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Lab_MDS_Primavera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
